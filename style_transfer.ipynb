{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we begin by importing all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "\n",
    "RESULT_PATH = '/home/dell-pc/Desktop/challenge-master/Model/'\n",
    "# change the above accordingly. Here the pretrained model is downloaded and final image is saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next are a few modules to load image, get features and computing losses. We shall use the VGG19 pretrained architecture for feature extraction and perform backpropogation on the target image pixels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size=400):\n",
    "  image = Image.open(img_path).convert('RGB')  \n",
    "  \n",
    "  if max(image.size) > max_size:\n",
    "    size = max_size\n",
    "  else:\n",
    "    size = max(image.size)\n",
    "\n",
    "  in_transform = transforms.Compose([\n",
    "    transforms.Resize((size, int(1.5*size))),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "  \n",
    "  image = in_transform(image)[:3, :, :].unsqueeze(0)\n",
    "  \n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_features(image, model, layers=None):\n",
    "  if layers is None:\n",
    "    layers = {'0': 'conv1_1','5': 'conv2_1',\n",
    "              '10': 'conv3_1',\n",
    "              '19': 'conv4_1',\n",
    "              '21': 'conv4_2',  \n",
    "              '28': 'conv5_1'}\n",
    "  features = {}\n",
    "  x = image\n",
    "  for name, layer in enumerate(model.features):\n",
    "    x = layer(x)\n",
    "    if str(name) in layers:\n",
    "      features[layers[str(name)]] = x\n",
    "  \n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "  _, n_filters, h, w = tensor.size()\n",
    "  tensor = tensor.view(n_filters, h * w)\n",
    "  gram = torch.mm(tensor, tensor.t())\n",
    "  \n",
    "  return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "  image = tensor.to(\"cpu\").clone().detach()\n",
    "  image = image.numpy().squeeze()\n",
    "  image = image.transpose(1, 2, 0)\n",
    "  image = image * np.array((0.229, 0.224, 0.225)) + np.array(\n",
    "    (0.485, 0.456, 0.406))\n",
    "  image = image.clip(0, 1)\n",
    "  \n",
    "  return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tv_loss(img):\n",
    "\n",
    "    w_variance = torch.sum(torch.pow(img[:,:,:,:-1] - img[:,:,:,1:], 2))\n",
    "    h_variance = torch.sum(torch.pow(img[:,:,:-1,:] - img[:,:,1:,:], 2))\n",
    "    loss = h_variance + w_variance\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now loading the pretrained model and computing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.utils.model_zoo.load_url('https://download.pytorch.org/models/vgg19-dcbb9e9d.pth', model_dir=RESULT_PATH)\n",
    "vgg = models.vgg19()\n",
    "vgg.load_state_dict(torch.load(RESULT_PATH + '/vgg19-dcbb9e9d.pth'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg.to(device).eval()\n",
    "content = load_image('japanese_garden.jpg').to(device)\n",
    "style = load_image('picasso_selfportrait.jpg').to(device)\n",
    "content_features = get_features(content, vgg)\n",
    "style_features = get_features(style, vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing weights as backprop is on image pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in vgg.parameters():\n",
    "  param.requires_grad_(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using average pool to replace max pool to ensure smoothness in final image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(vgg.features):\n",
    "  if isinstance(layer, torch.nn.MaxPool2d):\n",
    "    vgg.features[i] = torch.nn.AvgPool2d(kernel_size=2, stride=2, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_grams = {\n",
    "  layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "target = torch.randn_like(content).requires_grad_(True).to(device)\n",
    "# for various weightage for style with lower layers getting more weights\n",
    "style_weights = {'conv1_1': 0.75,\n",
    "                 'conv2_1': 0.5,\n",
    "                 'conv3_1': 0.2,\n",
    "                 'conv4_1': 0.2,\n",
    "                 'conv5_1': 0.2}\n",
    "\n",
    "# these weights are for their contribution in final loss function\n",
    "content_weight = 1e2\n",
    "style_weight = 1\n",
    "variation_weight = 1e-4\n",
    "optimizer = optim.Adam([target], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, Total loss: 1095.49 - (content: 0.59, style 0.25, variation 0.16)\n",
      "Iteration 20, Total loss: 804.37 - (content: 0.58, style 0.27, variation 0.15)\n",
      "Iteration 30, Total loss: 626.42 - (content: 0.56, style 0.3, variation 0.14)\n",
      "Iteration 40, Total loss: 524.41 - (content: 0.55, style 0.32, variation 0.13)\n",
      "Iteration 50, Total loss: 461.52 - (content: 0.55, style 0.33, variation 0.12)\n",
      "Iteration 60, Total loss: 417.18 - (content: 0.54, style 0.34, variation 0.12)\n",
      "Iteration 70, Total loss: 385.04 - (content: 0.54, style 0.34, variation 0.12)\n",
      "Iteration 80, Total loss: 360.33 - (content: 0.54, style 0.35, variation 0.12)\n",
      "Iteration 90, Total loss: 341.51 - (content: 0.54, style 0.35, variation 0.12)\n",
      "Iteration 100, Total loss: 326.91 - (content: 0.53, style 0.35, variation 0.12)\n",
      "Iteration 110, Total loss: 315.27 - (content: 0.53, style 0.35, variation 0.12)\n",
      "Iteration 120, Total loss: 306.09 - (content: 0.53, style 0.36, variation 0.12)\n",
      "Iteration 130, Total loss: 298.04 - (content: 0.53, style 0.36, variation 0.12)\n",
      "Iteration 140, Total loss: 291.72 - (content: 0.52, style 0.36, variation 0.12)\n",
      "Iteration 150, Total loss: 286.29 - (content: 0.52, style 0.36, variation 0.12)\n",
      "Iteration 160, Total loss: 281.73 - (content: 0.52, style 0.36, variation 0.12)\n",
      "Iteration 170, Total loss: 277.81 - (content: 0.52, style 0.37, variation 0.12)\n",
      "Iteration 180, Total loss: 274.39 - (content: 0.51, style 0.37, variation 0.12)\n",
      "Iteration 190, Total loss: 270.84 - (content: 0.51, style 0.37, variation 0.12)\n",
      "Iteration 200, Total loss: 268.03 - (content: 0.51, style 0.37, variation 0.12)\n",
      "Iteration 210, Total loss: 265.77 - (content: 0.51, style 0.38, variation 0.12)\n",
      "Iteration 220, Total loss: 263.6 - (content: 0.5, style 0.38, variation 0.12)\n",
      "Iteration 230, Total loss: 261.56 - (content: 0.5, style 0.38, variation 0.12)\n",
      "Iteration 240, Total loss: 259.69 - (content: 0.5, style 0.38, variation 0.12)\n",
      "Iteration 250, Total loss: 257.61 - (content: 0.5, style 0.38, variation 0.12)\n",
      "Iteration 260, Total loss: 256.31 - (content: 0.5, style 0.38, variation 0.12)\n",
      "Iteration 270, Total loss: 254.97 - (content: 0.49, style 0.38, variation 0.12)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 401):\n",
    "  \n",
    "  optimizer.zero_grad()\n",
    "  target_features = get_features(target, vgg)\n",
    "  \n",
    "  content_loss = torch.mean((target_features['conv4_2'] -\n",
    "                             content_features['conv4_2']) ** 2)\t\n",
    "  variation_loss = tv_loss(target)\n",
    "  \n",
    "  style_loss = 0\n",
    "  for layer in style_weights:\n",
    "    target_feature = target_features[layer]\n",
    "    target_gram = gram_matrix(target_feature)\n",
    "    _, d, h, w = target_feature.shape\n",
    "    style_gram = style_grams[layer]\n",
    "    layer_style_loss = style_weights[layer] * torch.mean(\n",
    "      (target_gram - style_gram) ** 2)\n",
    "    style_loss += layer_style_loss / (d * h * w)\n",
    "\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss + variation_weight*variation_loss\n",
    "    total_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "  if i % 10 == 0:\n",
    "    total_loss_rounded = round(total_loss.item(), 2)\n",
    "    tv_fraction = round(\n",
    "      variation_weight*variation_loss.item()/total_loss.item(), 2)\n",
    "    content_fraction = round(\n",
    "      content_weight*content_loss.item()/total_loss.item(), 2)\n",
    "    style_fraction = round(\n",
    "      style_weight*style_loss.item()/total_loss.item(), 2)\n",
    "    print('Iteration {}, Total loss: {} - (content: {}, style {}, variation {})'.format(\n",
    "      i,total_loss_rounded, content_fraction, style_fraction,tv_fraction))\n",
    "      \n",
    "final_img = im_convert(target)\n",
    "fig = plt.figure()\n",
    "plt.imshow(final_img)\n",
    "plt.axis('off')\n",
    "plt.savefig(RESULT_PATH+'final.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "1. A Neural Algorithm of Artistic Style by Gatys et al.(https://arxiv.org/pdf/1508.06576.pdf)\n",
    "2. https://www.researchgate.net/figure/Details-on-the-VGG19-architecture-For-each-layer-number-of-filters-parameters-and_tbl1_314237915\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
